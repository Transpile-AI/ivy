{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ivy\n",
    "import os\n",
    "import gc\n",
    "import abc\n",
    "import math\n",
    "import time\n",
    "import queue\n",
    "import psutil\n",
    "import inspect\n",
    "import logging\n",
    "import nvidia_smi\n",
    "from typing import Optional\n",
    "\n",
    "from typing import Optional, Union, Tuple, Sequence\n",
    "\n",
    "# local\n",
    "from ivy.backend_handler import current_backend\n",
    "from ivy.func_wrapper import (\n",
    "    infer_device,\n",
    "    infer_dtype,\n",
    "    outputs_to_ivy_arrays,\n",
    "    handle_out_argument,\n",
    "    to_native_arrays_and_back,\n",
    "    handle_nestable,\n",
    ")\n",
    "\n",
    "def used_mem_on_dev(\n",
    "    device: Union[ivy.Device, ivy.NativeDevice], process_specific=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the used memory (in GB) for a given device string. In case of CPU, the used\n",
    "    RAM is returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    device\n",
    "        The device string to convert to native device handle.\n",
    "    process_specific\n",
    "        Whether the check the memory used by this python process alone. Default is\n",
    "        False.\n",
    "        \n",
    "    Return type\n",
    "        Float\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    ret\n",
    "        The used memory on the device in GB.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    A \"cpu\" as device string:\n",
    "    >>> x = ivy.as_native_dev(\"cpu\") as device\n",
    "    >>> ivy.used_mem_on_dev(x)\n",
    "    \n",
    "    A \"gpu\" as device string:\n",
    "    >>> y = ivy.as_native_dev(\"gpu:idx\") as device\n",
    "    >>> ivy.used_mem_on_dev(y)\n",
    "    \n",
    "    >>> ivy.set_backend(\"torch\")\n",
    "    >>> z = ivy.as_native_dev(\"cpu\")\n",
    "    >>> ivy.used_mem_on_dev(z)\n",
    "    \n",
    "    >>> import torch\n",
    "    >>> ivy.set_backend(\"torch\")\n",
    "    >>> device = torch.device(\"cpu\")\n",
    "    >>> ivy.default_device(as_native=True)\n",
    "    >>> ivy.used_mem_on_dev(device)\n",
    "    \"\"\"\n",
    "    ivy.clear_mem_on_dev(device)\n",
    "    if \"gpu\" in device:\n",
    "        if process_specific:\n",
    "            raise Exception(\"process-specific GPU queries are currently not supported\")\n",
    "            handle = _get_nvml_gpu_handle(device)\n",
    "            info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "            return info.used / 1e9\n",
    "    elif device == \"cpu\":\n",
    "        if process_specific:\n",
    "            return psutil.Process(os.getpid()).memory_info().rss\n",
    "            vm = psutil.virtual_memory()\n",
    "            return (vm.total - vm.available) / 1e9\n",
    "    else:\n",
    "        raise Exception(\n",
    "            'Invalid device string input, must be on the form \"gpu:idx\" or \"cpu\", '\n",
    "            \"but found {}\".format(device)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ivy_dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8651079cf8432928cf42ab2d965f9d95476ac72cdf8539823f693eda6764eb44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
